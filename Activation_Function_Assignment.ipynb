{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf9ed274-0bad-4d54-a612-3b1bbcc719ed",
   "metadata": {},
   "source": [
    "# ANSWER 1\n",
    "An activation function is a mathematical function that determines the output of a neuron in an artificial neural network based on its input. It adds non-linearity to the network, enabling it to capture complex relationships in data. The activation function decides whether a neuron should be activated (fire) or not, based on the input it receives. Activation functions introduce non-linearities, allowing neural networks to model a wide range of functions and patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134384be-8daa-41eb-b2b4-00bc85fe64cf",
   "metadata": {},
   "source": [
    "# ANSWER 2\n",
    "Some common activation functions include:\n",
    "1. Sigmoid\n",
    "2. Hyperbolic tangent (tanh)\n",
    "3. Rectified Linear Unit (ReLU)\n",
    "4. Leaky ReLU\n",
    "5. Parametric ReLU (PReLU)\n",
    "6. Exponential Linear Unit (ELU)\n",
    "7. Swish\n",
    "8. Softmax (used in the output layer for multi-class classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beab0b3f-c929-4bdc-9430-2abc68d29221",
   "metadata": {},
   "source": [
    "# ANSWER 3\n",
    "Activation functions play a crucial role in the training process and performance of neural networks. They affect how information flows through the network and can impact convergence speed, vanishing/exploding gradient problems, and overall accuracy. Choosing appropriate activation functions for each layer can help stabilize training and improve the network's ability to learn complex patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8791d1eb-9bab-4d8f-b6dc-eb9bb33ceff6",
   "metadata": {},
   "source": [
    "# ANSWER 4\n",
    "The sigmoid activation function maps input values to a range between 0 and 1. It's given by the formula: f(x) = 1 / (1 + exp(-x)). Advantages include its smoothness and output in a interpretable probability-like range, which makes it suitable for binary classification. However, it suffers from vanishing gradients for large input values, slowing down training and limiting its effectiveness in deep networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d7e458-5f7b-49ca-a33e-d90753246d72",
   "metadata": {},
   "source": [
    "# ANSWER 5\n",
    "ReLU activation (f(x) = max(0, x)) outputs the input as-is if it's positive, and zero if it's negative. Unlike sigmoid, ReLU is not bound to a specific range and avoids vanishing gradients for positive inputs. It's computationally efficient and has been instrumental in training deep networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b155306-1655-4c80-ada6-849a110d1f3f",
   "metadata": {},
   "source": [
    "# ANSWER 6\n",
    "ReLU addresses the vanishing gradient problem more effectively, allowing for faster convergence during training. It also promotes sparse activations, making the network more efficient by reducing the likelihood of unnecessary computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833149bb-9687-4895-9106-4045fa2b36d6",
   "metadata": {},
   "source": [
    "# ANSWER 7\n",
    "Leaky ReLU is a variation of ReLU (f(x) = x if x > 0, and f(x) = ax if x <= 0 where a is a small positive constant). It introduces a small slope for negative inputs, preventing complete deactivation and mitigating the vanishing gradient issue. This can help networks converge faster and perform better on certain tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d3e2f2-4b77-4318-84e4-d4369a8df429",
   "metadata": {},
   "source": [
    "# ANSWER 8\n",
    "Softmax is used in the output layer of a neural network for multi-class classification tasks. It converts a vector of raw scores into a probability distribution over multiple classes, allowing the network to output the class probabilities. It's commonly used in scenarios where an input belongs to one of several possible classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6e9369-0e95-4f1c-be6e-fafd7f1dc7ff",
   "metadata": {},
   "source": [
    "# ANSWER 9\n",
    "The hyperbolic tangent activation function (f(x) = tanh(x)) is similar to the sigmoid function but maps input values to a range between -1 and 1. Like sigmoid, tanh suffers from vanishing gradients, particularly for inputs far from zero. However, it's zero-centered, which can help with training deep networks as it maintains both positive and negative output values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba85bb5c-2442-4b1d-a924-19e1f0e801d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
